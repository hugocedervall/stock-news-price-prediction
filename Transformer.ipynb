{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "if len(tf.config.list_physical_devices('GPU')):\n",
    "    physical_devices = tf.config.list_physical_devices('GPU')\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30220 summaries containing 'No summary available'\n",
      "6142 summaries containing 'Full story available on'\n",
      "Duplicate summaries removed: 14242\n",
      "Duplicate headlines removed: 404\n",
      "randomizing order of data\n"
     ]
    }
   ],
   "source": [
    "from data_getter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "496"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "max_len = np.array([len(sentence.split(\" \")) for sentence in balanced_dataset.summary])\n",
    "np.max(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([32900.,     0.,     0.,     0.,     0., 32900.,     0.,     0.,\n",
       "            0., 32900.]),\n",
       " array([0. , 0.2, 0.4, 0.6, 0.8, 1. , 1.2, 1.4, 1.6, 1.8, 2. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(balanced_dataset.priceChangeClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot = plt.hist(sorted(max_len), bins=50, density=True, range=[0,512])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the graph, a sequence length of 256 captures the majority of features. \n",
    "To be exact:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99596757852077"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(max_len <= 256) / len(max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertForSequenceClassification,DistilBertTokenizer\n",
    "\n",
    "#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True)\n",
    "#model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'stock', 'tumbled', 'over', '4', 'per', 'cent', 'to', 'rs', '119', '.', '30', 'on', 'the', 'bs', '##e', '.']\n",
      "[1996, 4518, 18303, 2058, 1018, 2566, 9358, 2000, 12667, 13285, 1012, 2382, 2006, 1996, 18667, 2063, 1012]\n"
     ]
    }
   ],
   "source": [
    "tokenized = tokenizer.tokenize(train_data.iloc[1].summary)\n",
    "print(tokenized)\n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokenized)\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_bert_input(df, tokenizer):\n",
    "    res = []\n",
    "    \n",
    "    for index,row in df.iterrows():\n",
    "        if index % 5000 == 0:\n",
    "            print(index/len(df.index)*100)\n",
    "        input_dict = tokenizer.encode_plus(\n",
    "                row.summary,\n",
    "                add_special_tokens=True,\n",
    "                max_length= 256, # truncates if len(s) > max_length\n",
    "                return_token_type_ids=True,\n",
    "                return_attention_mask=True,\n",
    "                truncation=True\n",
    "            )\n",
    "        res.append(input_dict[\"input_ids\"])\n",
    "    return np.array(res)\n",
    "\n",
    "\n",
    "def transform_input(df):\n",
    "    return tokenizer.batch_encode_plus(\n",
    "            df.summary,\n",
    "            return_tensors='tf',\n",
    "            add_special_tokens=True,\n",
    "            max_length= 256, # truncates if len(s) > max_length\n",
    "            return_token_type_ids=True,\n",
    "            padding='max_length',\n",
    "            truncation=True)\n",
    "\n",
    "def transform_input2(df):\n",
    "    return tokenizer(df.summary.tolist(), \n",
    "                     truncation=True, \n",
    "                     padding=True, \n",
    "                     max_length=256,\n",
    "                     return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Portland General Electric (NYSE:POR) â€“ KeyCorp...\n",
       "1        The stock tumbled over 4 per cent to Rs 119.30...\n",
       "2        Leslie Wexner is the CEO of L Brands, which in...\n",
       "3        NORFOLK, Neb.--(BUSINESS WIRE)---- $CDOR #Hote...\n",
       "4        From Louis Vuitton luggage to a watch that pro...\n",
       "                               ...                        \n",
       "71059    Growing Ethereum network transaction fees, whi...\n",
       "71060    BAE Systems will produce multiple types of Ver...\n",
       "71061    HARRISBURG, Pa., Dec. 5, 2019 /PRNewswire/ -- ...\n",
       "71062    Skechers (SKX) shuts company-owned retail stor...\n",
       "71063    Wheaton Precious Metals (NYSE: WPM) shares are...\n",
       "Name: summary, Length: 71064, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#x_train_trans = convert_to_bert_input(train_data, tokenizer)\n",
    "#x_valid_trans = convert_to_bert_input(validation_data, tokenizer)\n",
    "\n",
    "train_data.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data.summary.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tmp = validation_data#.sample(n=1000)\n",
    "x_valid_trans = transform_input2(x_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtr_tmp = train_data#.sample(n=5000)\n",
    "x_train_trans = transform_input2(xtr_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(71064, 256), dtype=int32, numpy=\n",
       "array([[  101,  6734,  2236, ...,     0,     0,     0],\n",
       "       [  101,  1996,  4518, ...,     0,     0,     0],\n",
       "       [  101,  8886,  2057, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [  101, 24569,  1010, ...,     0,     0,     0],\n",
       "       [  101, 15315, 27635, ...,     0,     0,     0],\n",
       "       [  101, 10500,  2239, ...,     0,     0,     0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(71064, 256), dtype=int32, numpy=\n",
       "array([[1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train_dataset = dataset = tf.data.Dataset.from_tensor_slices((dict(x_train_trans['input_ids'],\n",
    "#                                              x_train_trans['attention_mask'],\n",
    "#                                              x_train_trans['token_type_ids']), y_train))\n",
    "#x_valid_dataset = dataset = tf.data.Dataset.from_tensor_slices((x_valid_trans['input_ids'],\n",
    "#                                              x_valid_trans['attention_mask'],\n",
    "#                                              x_valid_trans['token_type_ids'], val_y))\n",
    "out_x_valid = np.array(x_tmp.priceChangeClass.tolist()).reshape((len(x_tmp),))\n",
    "out_x_train = np.array(xtr_tmp.priceChangeClass.tolist()).reshape((len(xtr_tmp),))\n",
    "\n",
    "x_valid_dataset = tf.data.Dataset.from_tensor_slices((dict(x_valid_trans), out_x_valid))\n",
    "x_train_dataset = tf.data.Dataset.from_tensor_slices((dict(x_train_trans), out_x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71064"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9870 <TensorSliceDataset shapes: ({input_ids: (256,), attention_mask: (256,)}, ()), types: ({input_ids: tf.int32, attention_mask: tf.int32}, tf.int64)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: ({input_ids: (256,), attention_mask: (256,)}, ()), types: ({input_ids: tf.int32, attention_mask: tf.int32}, tf.int64)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(x_valid_dataset), x_valid_dataset)\n",
    "x_valid_dataset\n",
    "#x_valid_dataset.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['activation_13', 'vocab_transform', 'vocab_projector', 'vocab_layer_norm']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier', 'dropout_19', 'classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFDistilBertForSequenceClassification, TFPreTrainedModel\n",
    "\n",
    "num_classes = len(set(balanced_dataset.priceChangeClass))\n",
    "\n",
    "#model = TFDistilBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
    "#model = TFPreTrainedModel.from_pretrained('distilbert-base-uncased', num_labels=3)\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=num_classes)\n",
    "\n",
    "print(num_classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2719/23688 [==>...........................] - ETA: 55:02 - loss: 1.1034 - accuracy: 0.3301"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-7fe506f80653>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m               metrics=tf.keras.metrics.SparseCategoricalAccuracy('accuracy', dtype=tf.float32))\n\u001b[1;32m      9\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m history = model.fit(x_train_dataset.batch(batch_size), \n\u001b[0m\u001b[1;32m     11\u001b[0m               \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m               \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/stock-news/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/stock-news/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/stock-news/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/stock-news/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/stock-news/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/stock-news/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m     \"\"\"\n\u001b[0;32m-> 1843\u001b[0;31m     return self._call_flat(\n\u001b[0m\u001b[1;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[1;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[0;32m~/.virtualenvs/stock-news/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/.virtualenvs/stock-news/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/stock-news/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.distilbert.return_dict = False\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "model.compile(optimizer=optimizer, \n",
    "              loss=loss, \n",
    "              metrics=tf.keras.metrics.SparseCategoricalAccuracy('accuracy', dtype=tf.float32))\n",
    "batch_size = 3\n",
    "history = model.fit(x_train_dataset.batch(batch_size), \n",
    "              epochs=1, \n",
    "              batch_size=batch_size, \n",
    "              validation_data=x_valid_dataset.batch(batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_distil_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "distilbert (TFDistilBertMain multiple                  66362880  \n",
      "_________________________________________________________________\n",
      "pre_classifier (Dense)       multiple                  590592    \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  2307      \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         multiple                  0         \n",
      "=================================================================\n",
      "Total params: 66,955,779\n",
      "Trainable params: 66,955,779\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def build_classifier_model(num_classes):\\n  inputs = dict(\\n      input_word_ids=tf.keras.layers.Input(shape=(None,), dtype=tf.int32),\\n      input_mask=tf.keras.layers.Input(shape=(None,), dtype=tf.int32),\\n      input_type_ids=tf.keras.layers.Input(shape=(None,), dtype=tf.int32),\\n  )\\n\\n  net = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=3)\\n  net = tf.keras.layers.Dense(num_classes,  name='classifier')(net)\\n\\n  return tf.keras.Model(inputs, net, name='prediction')\\nmodel = build_classifier_model(3)\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def build_classifier_model(num_classes):\n",
    "  inputs = dict(\n",
    "      input_word_ids=tf.keras.layers.Input(shape=(None,), dtype=tf.int32),\n",
    "      input_mask=tf.keras.layers.Input(shape=(None,), dtype=tf.int32),\n",
    "      input_type_ids=tf.keras.layers.Input(shape=(None,), dtype=tf.int32),\n",
    "  )\n",
    "\n",
    "  net = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=3)\n",
    "  net = tf.keras.layers.Dense(num_classes,  name='classifier')(net)\n",
    "\n",
    "  return tf.keras.Model(inputs, net, name='prediction')\n",
    "model = build_classifier_model(3)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#net = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=3)\n",
    "#model = tf.keras.layers.Dense(3)(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.modeling_tf_outputs.TFSequenceClassifierOutput'>\n",
      "[[-0.3038873   0.14581959  0.02287905]]\n",
      "[[0.25288734 0.39649007 0.3506226 ]]\n"
     ]
    }
   ],
   "source": [
    "i = next(iter(x_train_dataset))[0]\n",
    "i[\"input_ids\"] = tf.reshape(i[\"input_ids\"],(1,256))\n",
    "i[\"attention_mask\"] = tf.reshape(i[\"attention_mask\"],(1,256))\n",
    "\n",
    "#print(i)\n",
    "\n",
    "tf_output = model.predict(i)\n",
    "print(type(tf_output))\n",
    "print(tf_output.logits)\n",
    "tf_prediction = tf.nn.softmax(tf_output.logits, axis=1).numpy()\n",
    "print(tf_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_distil_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "distilbert (TFDistilBertMain multiple                  66362880  \n",
      "_________________________________________________________________\n",
      "pre_classifier (Dense)       multiple                  590592    \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  2307      \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         multiple                  0         \n",
      "=================================================================\n",
      "Total params: 66,955,779\n",
      "Trainable params: 66,955,779\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers==4.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip freeze | grep transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length_test = 256\n",
    "test_sentence = 'Test tokenization sentence. Followed by another sentence'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized ['value', '##ng', '##ine', 'upgraded', 'shares', 'of', 'sk', '##eche', '##rs', 'usa', '(', 'ny', '##se', ':', 'sk', '##x', ')', 'from', 'a', 'hold', 'rating', 'to', 'a', 'buy', 'rating', 'in', 'a', 'research', 'note', 'issued', 'to', 'investors', 'on', 'saturday', 'morning', ',']\n",
      "{'token_ids': [3643, 3070, 3170, 9725, 6661, 1997, 15315, 27635, 2869, 3915, 1006, 6396, 3366, 1024, 15315, 2595, 1007, 2013, 1037, 2907, 5790, 2000, 1037, 4965, 5790, 1999, 1037, 2470, 3602, 3843, 2000, 9387, 2006, 5095, 2851, 1010, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "test_text = \"ValuEngine upgraded shares of Skechers USA (NYSE:SKX) from a hold rating to a buy rating in a research note issued to investors on Saturday morning,\"\n",
    "tokenized = tokenizer.tokenize(test_text)\n",
    "print('tokenized', tokenized)\n",
    "\n",
    "# convert tokens to ids in WordPiece\n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokenized)\n",
    "  \n",
    "# precalculation of pad length, so that we can reuse it later on\n",
    "padding_length = max_length_test - len(input_ids)\n",
    "\n",
    "# map tokens to WordPiece dictionary and add pad token for those text shorter than our max length\n",
    "input_ids = input_ids + ([0] * padding_length)\n",
    "\n",
    "# attention should focus just on sequence with non padded tokens\n",
    "attention_mask = [1] * len(input_ids)\n",
    "\n",
    "# do not focus attention on padded tokens\n",
    "attention_mask = attention_mask + ([0] * padding_length)\n",
    "\n",
    "# token types, needed for example for question answering, for our purpose we will just set 0 as we have just one sequence\n",
    "token_type_ids = [0] * max_length_test\n",
    "bert_input = {\n",
    "    \"token_ids\": input_ids,\n",
    "    \"token_type_ids\": token_type_ids,\n",
    "    \"attention_mask\": attention_mask\n",
    "} \n",
    "print(bert_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded {'input_ids': [101, 3231, 19204, 3989, 6251, 1012, 2628, 2011, 2178, 6251, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hugo/.virtualenvs/stock-news/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2173: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "bert_input = tokenizer.encode_plus(\n",
    "                        test_sentence,                      \n",
    "                        add_special_tokens = True, # add [CLS], [SEP]\n",
    "                        max_length = max_length_test, # max length of the text that can go to BERT\n",
    "                        pad_to_max_length = True, # add [PAD] tokens\n",
    "                        return_attention_mask = True, # add attention mask to not focus on pad tokens\n",
    "              )\n",
    "print('encoded', bert_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "if len(tf.config.list_physical_devices('GPU')):\n",
    "    physical_devices = tf.config.list_physical_devices('GPU')\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFBertForSequenceClassification\n",
    "import tensorflow as tf\n",
    "\n",
    "# recommended learning rate for Adam 5e-5, 3e-5, 2e-5\n",
    "learning_rate = 2e-5\n",
    "# we will do just 1 epoch for illustration, though multiple epochs might be better as long as we will not overfit the model\n",
    "number_of_epochs = 1\n",
    "\n",
    "# model initialization\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# optimizer Adam recommended\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=1e-08)\n",
    "\n",
    "# we do not have one-hot vectors, we can use sparce categorical cross entropy and accuracy\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_history = model.fit(ds_train_encoded, epochs=number_of_epochs, validation_data=ds_test_encoded)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
